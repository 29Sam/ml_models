1)logistic regression -(despite the name)used for binary classification tasks 
claases are defined as p[ositive(p) or nagative class(1-p) .

for finding p ie P(p/i)   where i is the input
first there is this linear regression part in which value giooes from -inf to +inf 
and then we apply a sigmid function to it which brings it value to 0 to 1


then there is some threshold to decide whether thins go intp positive class or neagtive class 
eg) if P(p/i)>o.5   -> positive class else negative 

error function used - cross entropy loss . 
(how different are the predicted probabilities are from the actual labels in a cassification problems )

2)multinomial logistic regression - extension of the binary classification of course 
softmax instead of the sigmoiud function 
each class prediction is learnt separately(neural netwrok method)

3) Naive bayes 
based on bayes thm 
naive because of assuming that features are conditionally independent(ee325) given the class label .
gaussian naive bayes , bernouli and multinomial naive bias are some exampkes 
(was very popular for spam detection once)

